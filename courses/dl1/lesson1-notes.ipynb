{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification with Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "- [Lesson 1](http://course.fast.ai/lessons/lesson1.html)\n",
    "- [Wiki: Lesson 1](http://forums.fast.ai/t/wiki-lesson-1/9398)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-down vs Bottom-up approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Screen Shot 2018-06-06 at 6.47.58 PM.png\">\n",
    "\n",
    "*Screenshot taken from [course.fast.ai](https://www.youtube.com/watch?v=IPBSB1HLNLo) 33:42*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Screen Shot 2018-06-06 at 6.49.43 PM.png\">\n",
    "\n",
    "*Screenshot taken from [course.fast.ai](https://www.youtube.com/watch?v=IPBSB1HLNLo) 34:49*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The idea of deep learning\n",
    "\n",
    "<img src=\"images/Screen Shot 2018-06-06 at 6.57.55 PM.png\">\n",
    "\n",
    "*Screenshot taken from [course.fast.ai](https://www.youtube.com/watch?v=IPBSB1HLNLo) 48:49*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Traditional machine learning is very time consumming. \n",
    "- The underlying function that deep learning uses is called Neural Network.\n",
    "    - it consists of a number of simple linear layers interspersed with a number of simple nonlinear layers\n",
    "    - when you in dispersed these layers in this way you get something called the universal approximation theorem\n",
    "    - __the universal approximation theorem__ says that this kind of function can solve any given problem to arbitrarily close accuracy as long as you add enough parameters so it's actually provably shown to be an infinitely flexible function\n",
    "    \n",
    "\n",
    "<img src=\"images/Screen Shot 2018-06-06 at 7.00.56 PM.png\">\n",
    "\n",
    "*Screenshot taken from [course.fast.ai](https://www.youtube.com/watch?v=IPBSB1HLNLo) 49:50*\n",
    "\n",
    "<!--TEASER_END-->\n",
    "\n",
    "- We need some way to fit the parameters so that this infinitely flexible neural network solves some specific problem and the way we do that is using a technique called __gradient descent__\n",
    "- With __gradient descent__ we say for the different parameters how good are they at solving my problem and let's figure out a slightly better set of parameters by following the surface of the loss function downward.\n",
    "\n",
    "<img src=\"images/Screen Shot 2018-06-06 at 7.06.03 PM.png\">\n",
    "\n",
    "*Screenshot taken from [course.fast.ai](https://www.youtube.com/watch?v=IPBSB1HLNLo) 50:20*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning applications\n",
    "\n",
    "<img src=\"images/Screen Shot 2018-06-06 at 7.14.16 PM.png\">\n",
    "\n",
    "*Screenshot taken from [course.fast.ai](https://www.youtube.com/watch?v=IPBSB1HLNLo) 58:50*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
